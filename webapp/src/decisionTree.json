{
  "start": {
    "type": "question",
    "questionText": "What is your primary privacy risk?",
    "iconId": "Scale",
    "options": [
      { "text": "Protecting Individual Training Data", "nextStep": "dataSensitivity", "iconId": "Database" },
      { "text": "Protecting the Model Itself", "nextStep": "modelProtectionPath", "iconId": "BrainCircuit" },
      { "text": "Protecting Inference Data", "nextStep": "privateInferencePath", "iconId": "Users" },
      { "text": "Ensuring Regulatory Compliance", "nextStep": "compliancePath", "iconId": "FileText" }
    ]
  },
  "dataSensitivity": {
    "type": "question",
    "questionText": "How sensitive is your training data?",
    "iconId": "Shield",
    "tooltipText": "Data sensitivity determines the level of noise needed. More sensitive data requires stronger privacy guarantees (lower ε).",
    "options": [
      { "text": "High (medical, financial)", "nextStep": "deploymentEnvironment", "icon": { "type": "styledText", "text": "High", "className": "text-red-500 font-bold" }, "context": { "sensitivity": "High", "epsilon": "ε ≤ 1.0" } },
      { "text": "Medium (behavioral, preferences)", "nextStep": "deploymentEnvironment", "icon": { "type": "styledText", "text": "Medium", "className": "text-yellow-500 font-bold" }, "context": { "sensitivity": "Medium", "epsilon": "ε = 3.0-5.0" } },
      { "text": "Low (non-personal, aggregate)", "nextStep": "deploymentEnvironment", "icon": { "type": "styledText", "text": "Low", "className": "text-green-500 font-bold" }, "context": { "sensitivity": "Low", "epsilon": "ε = 8.0-10.0" } }
    ]
  },
  "deploymentEnvironment": {
    "type": "question",
    "questionText": "Where will your ML system be deployed?",
    "iconId": "Cpu",
    "options": [
      { "text": "On client devices (phones, IoT)", "nextStep": "solution_federated", "iconId": "Smartphone" },
      { "text": "Trusted centralized server", "nextStep": "solution_central_dp", "iconId": "Server" },
      { "text": "Untrusted server/cloud", "nextStep": "solution_crypto", "iconId": "Lock" },
      { "text": "Multi-party collaboration", "nextStep": "solution_smpc", "iconId": "Group" }
    ]
  },
  "solution_federated": {
    "type": "solution",
    "title": "Federated Learning Path",
    "iconId": "Smartphone",
    "description": "For systems running on client devices, Federated Learning (FL) is the ideal architecture. Data stays on the user's device, preserving privacy by design.",
    "recommendations": [
      { "text": "<strong>Privacy Context</strong>: You selected <strong>{{sensitivity}}</strong> sensitivity data. Aim for a privacy budget of <strong>{{epsilon}}</strong> when applying local DP.", "requiresContext": "epsilon" },
      { "text": "<strong>Need formal privacy guarantees?</strong> Add Local Differential Privacy. Each client adds noise to its model update before sending it to the server. This protects individual user data even from the server." },
      { "text": "<strong>Want protection from a curious server?</strong> Implement a Secure Aggregation protocol. This cryptographic technique ensures the server can only learn the sum of all model updates, not individual contributions." },
      { "text": "<strong>Communication-constrained?</strong> FL can be bandwidth-intensive. Consider model compression techniques (e.g., quantization, sparsification) or more communication-efficient FL algorithms." },
      { "text": "<strong>Recommended Tools</strong>: Use frameworks like TensorFlow Federated (TFF) or Flower (PyTorch-based) to implement your FL system." }
    ]
  },
  "solution_central_dp": {
    "type": "solution",
    "title": "Central Differential Privacy Path",
    "iconId": "Server",
    "description": "When you have a trusted server that can access raw data, Central DP provides strong, formal privacy guarantees for the entire dataset.",
    "recommendations": [
        { "text": "<strong>Privacy Context</strong>: You selected <strong>{{sensitivity}}</strong> sensitivity data. The central privacy budget for the entire training process should be <strong>{{epsilon}}</strong>.", "requiresContext": "epsilon" },
        { "text": "<strong>Training neural networks?</strong> Use Differentially Private Stochastic Gradient Descent (DP-SGD). It clips the influence of each data point and adds noise during training." },
        { "text": "<strong>Working with structured data?</strong> Use differentially private query mechanisms or algorithms designed for tabular data (e.g., releasing private histograms or synthetic data)." },
        { "text": "<strong>Need high accuracy?</strong> DP introduces a privacy-utility trade-off. Use tight composition bounds (e.g., Rényi DP) to more accurately track the privacy budget over many training steps, often leading to better utility." },
        { "text": "<strong>Recommended Tools</strong>: Use Opacus (for PyTorch) or TensorFlow Privacy for robust and tested implementations of DP-SGD." }
    ]
  },
  "solution_crypto": {
    "type": "solution",
    "title": "Cryptography Path for Untrusted Servers",
    "iconId": "Lock",
    "description": "For systems on untrusted servers, cryptographic methods allow computation on data without revealing it to the server.",
    "recommendations": [
        { "text": "<strong>Simple operations or linear models?</strong> Use Homomorphic Encryption (HE). It allows the server to compute on encrypted data. This is computationally very expensive and best suited for simpler models." },
        { "text": "<strong>Complex model needed?</strong> The performance of pure HE may be prohibitive. Consider hybrid approaches that combine HE with Secure Multi-Party Computation (SMPC) or run parts of the model on the client." },
        { "text": "<strong>Performance critical?</strong> Trusted Execution Environments (TEEs) like Intel SGX or AMD SEV can be an option. They create a secure enclave on the server's CPU where data can be decrypted and processed in isolation. Be aware of potential side-channel attacks." },
        { "text": "<strong>Recommended Tools</strong>: Microsoft SEAL or TenSEAL for Homomorphic Encryption; CrypTen for secure multi-party computation." }
    ]
  },
  "solution_smpc": {
    "type": "solution",
    "title": "Secure Multi-Party Computation (SMPC) Path",
    "iconId": "Group",
    "description": "SMPC is designed for scenarios where multiple organizations want to collaborate on training a model without sharing their private datasets.",
    "recommendations": [
        { "text": "<strong>Privacy Context</strong>: You selected <strong>{{sensitivity}}</strong> sensitivity data. Consider combining SMPC with DP techniques, applying a budget of <strong>{{epsilon}}</strong> to each party's data.", "requiresContext": "epsilon" },
        { "text": "<strong>Few parties (<10)?</strong> Protocols based on Garbled Circuits or Secret Sharing (like GMW) are often suitable and can provide strong security guarantees (malicious vs. semi-honest)." },
        { "text": "<strong>Many parties?</strong> As the number of parties grows, traditional SMPC becomes difficult. Federated Learning with Secure Aggregation is a more scalable form of SMPC designed for this scenario." },
        { "text": "<strong>Communication costs</strong>: SMPC protocols are communication-heavy. The choice of protocol depends on the trade-off between communication rounds, computational cost, and the desired security model." },
        { "text": "<strong>Recommended Tools</strong>: MP-SPDZ or CrypTen for general-purpose secure computation; TensorFlow Federated for large-scale collaboration." }
    ]
  },
  "modelProtectionPath": {
    "type": "solution",
    "title": "Model Protection Path",
    "iconId": "BrainCircuit",
    "description": "If protecting the model itself as intellectual property or from specific attacks is the primary goal.",
    "recommendations": [
        { "text": "<strong>Preventing model stealing?</strong> Implement watermarking by embedding a secret trigger-response pattern into your model. This helps prove ownership if it's stolen. Rate-limiting API access can also deter theft." },
        { "text": "<strong>Preventing membership inference?</strong> This attack aims to determine if a specific user's data was in the training set. Training with Differential Privacy is the most effective defense." },
        { "text": "<strong>Preventing model inversion/extraction?</strong> These attacks try to reconstruct training data or the model's logic. Defenses include adding noise to model outputs (output perturbation), reducing the confidence scores reported, and again, using Differential Privacy." },
        { "text": "<strong>Protecting model parameters?</strong> If the model parameters themselves must remain secret, consider using encrypted models (with HE/SMPC) or deploying the model within a Trusted Execution Environment (TEE)." }
    ]
  },
  "privateInferencePath": {
    "type": "solution",
    "title": "Private Inference Path",
    "iconId": "Users",
    "description": "If protecting the user's input data during prediction (inference) is the primary concern.",
    "recommendations": [
        { "text": "<strong>Local inference possible?</strong> The best solution is to run the model directly on the client's device (e.g., using TensorFlow Lite, Core ML). The data never leaves the device, providing maximum privacy." },
        { "text": "<strong>Need provable security on a server?</strong> Use Fully Homomorphic Encryption (FHE). The client sends encrypted data, the server computes the prediction on the ciphertext, and returns an encrypted result. It is very secure but also very slow." },
        { "text": "<strong>Balance of security/performance?</strong> Use hybrid cryptographic approaches like those from DELPHI or GAZELLE, which smartly combine HE with SMPC to achieve better performance than pure FHE." },
        { "text": "<strong>Performance critical?</strong> Use Trusted Execution Environments (TEEs) like Intel SGX. The client establishes a secure channel with the enclave on the server, sends its data, the model computes inside the enclave, and sends the result back. This is much faster than FHE but relies on hardware security." }
    ]
  },
  "compliancePath": {
    "type": "solution",
    "title": "Regulatory Compliance Path",
    "iconId": "FileText",
    "description": "If your primary driver is meeting specific legal or regulatory requirements for data privacy.",
    "recommendations": [
        { "text": "<strong>For GDPR (Europe)</strong>: Focus on 'Data Protection by Design'. Key principles are data minimization, purpose limitation, and user rights (e.g., Right to be Forgotten). Differential Privacy can be a strong technical measure to demonstrate anonymization. You must have a 'machine unlearning' strategy." },
        { "text": "<strong>For HIPAA (US Healthcare)</strong>: Focus on the de-identification of Protected Health Information (PHI). The 'Safe Harbor' or 'Expert Determination' methods are required. Federated Learning is a powerful architecture for HIPAA, as PHI never leaves the hospital's control." },
        { "text": "<strong>For CCPA/CPRA (California)</strong>: Focus on transparency and user rights, particularly the right to opt-out of data 'sharing' for advertising purposes and the right to deletion. Your data pipelines must be able to honor these flags." },
        { "text": "<strong>General Advice</strong>: For all regulations, <strong>Differential Privacy</strong> provides strong, quantifiable evidence that your system protects individual privacy. <strong>Synthetic Data</strong> generated with privacy guarantees can also be a valuable tool for analysis without using real user data. Always pair technical solutions with legal counsel and thorough documentation (e.g., Privacy Impact Assessments)." }
    ]
  }
}